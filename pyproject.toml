[project]
name = "gameplay-vision-llm"
version = "0.1.0"
description = "Multi-modal video LLM for gameplay analysis with SAM3, OCR, and Qwen3-VL reasoning"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}

dependencies = [
    # Core ML/DL
    "torch>=2.0.0",
    "torchvision",
    "torchaudio",
    "transformers @ git+https://github.com/huggingface/transformers.git@ff13eb668aa03f151ded71636d723f2e490ad967",
    "accelerate>=1.0.0",
    "safetensors",
    
    # LoRA / Fine-tuning
    "peft>=0.10.0",
    "bitsandbytes>=0.40.0",
    "datasets>=2.0.0",
    "trl>=0.7.0",
    
    # Vision
    "opencv-python-headless>=4.6.0",
    "pillow>=10.0.0",
    "decord>=0.6.0",
    "scikit-image",
    
    # OCR
    "paddlepaddle==2.6.2",  # CPU-only to avoid CUDNN conflicts
    "paddleocr>=2.7.0",
    
    # Audio
    "openai-whisper",
    "librosa>=0.10.0",
    "soundfile",
    
    # ML utilities
    "sentence-transformers>=2.0.0",
    "scikit-learn",
    "numpy>=1.20.0",
    "scipy",
    
    # Data processing
    "pandas",
    "pyarrow",
    "tqdm",
    "pyyaml",
    
    # HTTP/API
    "httpx",
    "requests",
    "huggingface_hub>=0.20.0",
    "hf_transfer",  # Fast HuggingFace downloads
    
    # Video download and processing
    "yt-dlp",
    "opencv-python>=4.6.0",
    
    # Additional ML/model dependencies
    "flash-attn",
    "einops",
    "timm",
    "sentencepiece",
    "tiktoken",
    
    # Visualization and graphs
    "matplotlib",
    "networkx",
]

[project.optional-dependencies]
dev = [
    "pytest",
    "black",
    "ruff",
    "ipython",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = [
    "src/agent_core",
    "src/audio",
    "src/fusion_indexing",
    "src/perception",
    "src/temporal",
]

[tool.uv]
dev-dependencies = [
    "pytest",
    "black",
    "ruff",
]

[tool.uv.extra-build-dependencies]
flash-attn = ["torch"]
